# Research summary

This document summarizes research into current LLM API patterns across major providers and universal abstraction attempts.

## Provider API Comparison

| Provider/System                 | Message Representation                                                                                          | Response Format                                                                                                 | Provider-Native Tools                                                                                       | Streaming Events                                                                                          | Reasoning/Thinking                                                                                                                                           | Context Caching                                                                                                                                                                              | Special Features                                                                             |
| ------------------------------- | --------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------- |
| **OpenAI Chat Completions API** | Standard `messages` array with role-based format: `system`, `user`, `assistant`, `tool`                         | Single message response with `choices[]` array, `usage` metadata, `finish_reason`                               | Client-managed tool calls only, no provider-native tools like web search                                    | Simple streaming with `data: [DONE]` termination                                                          | No reasoning support - just direct completions                                                                                                               | No built-in caching, stateless requests                                                                                                                                                      | Simple, stateless, widely adopted standard                                                   |
| **OpenAI Responses API**        | Moving from simple messages to complex item-based responses with stateful conversations                         | Supports both `List[Message]` (stateless) and enhanced response objects with metadata for stateful mode         | Provider-managed tools: web search ($25-50/1k), code interpreter, file search, computer use, MCP            | 25+ specific streaming events: `response.created`, `content_part.added`, `output_text.delta`, etc.        | **Encrypted reasoning items** - internal chain of thought tokens, mostly hidden for safety, accessible via `previous_response_id` for performance continuity | Stateful via `previous_response_id` input parameter, plus implicit caching                                                                                                                   | Stateful conversations, background processing, resumable streaming, sequence numbers         |
| **Anthropic Messages API**      | Role-based messages with content blocks: `text`, `image`, `document`, `tool_use`, `tool_result`, `thinking`     | Consistent message structure: `{id, role, content[], model, stop_reason, usage}` with detailed token metrics    | Provider-managed tools: web search ($10/1k), code execution, files API - with pause/resume via `pause_turn` | 6 main events: `message_start`, `content_block_delta`, `message_stop` - includes thinking block streaming | **Visible thinking blocks** - readable step-by-step reasoning streamed as content, `thinking` content type                                                   | Explicit `cache_control` with 5min/1hr options, prefix matching, 25% write premium, 10% read discount                                                                                        | System prompts (separate parameter), prefill capability, extended thinking modes             |
| **Google Gemini API**           | Role + parts array: `text`, `inlineData`, `fileData`, `functionCall`, `functionResponse`, `codeExecution`       | Candidates array with parts, safety ratings, citation metadata: `{candidates[], promptFeedback, usageMetadata}` | **Mixed**: Client-managed tool calls + provider-managed tools (search grounding, code execution, web fetch) | Chunked responses with partial content, real-time WebSocket for Live API                                  | Internal reasoning process, but not directly exposed as streamable content                                                                                   | **Stateful explicit caching** - separate `/cachedContents` API creates cache, reference via `cachedContent: cache.name` in subsequent calls. Also implicit caching (auto for 1k+/2k+ tokens) | Safety settings, generation config, multimodal-first architecture, Live API WebSocket        |
| **OpenRouter**                  | Normalizes everything to OpenAI Chat Completions format                                                         | Normalizes to OpenAI format with `choices[]` array, standardized finish reasons                                 | Client-managed tool calls only, no provider-native tools exposed                                            | Normalizes to simpler Server-Sent Events (SSE) streaming                                                  | Flattens reasoning to simple token streams                                                                                                                   | Provider-specific caching handled transparently                                                                                                                                              | Fallback/load balancing, provider routing, normalized error handling                         |
| **LiteLLM**                     | Uses OpenAI format as standard, also supports Anthropic `/messages` format                                      | OpenAI-compatible responses with normalized token counting                                                      | **Mixed**: Supports provider-managed tools (Anthropic), translates others to client-managed tool calls      | Unified streaming with `stream=True`, consistent across providers                                         | Handles reasoning through provider-specific pass-through                                                                                                     | Supports Anthropic's prompt caching via `cache_control`                                                                                                                                      | Exception mapping, capability detection, middleware pattern                                  |
| **Vercel AI SDK**               | Own universal message format with provider conversion                                                           | Type-safe responses with rich metadata and usage statistics                                                     | **Mixed**: Client-managed tool calls + provider-managed tools via `provider.tools.toolName()` syntax        | Abstracts to unified streaming interface with event callbacks                                             | Abstracts reasoning handling in unified interface                                                                                                            | Abstracts caching mechanisms across providers                                                                                                                                                | Framework integration, type safety, composable functions                                     |
| **LangChain**                   | Object-oriented messages: `SystemMessage`, `HumanMessage`, `AIMessage`, `ToolMessage` with content and metadata | `AIMessage` objects with consistent token usage metadata and tool calls across providers                        | **Mixed**: Client-managed tool calls via `bind_tools()` + provider-managed tools (varies by provider)       | `AIMessageChunk` objects that merge with `+` operator, `astream_events()` for advanced streaming          | Recent support for Claude 3.7 thinking blocks and OpenAI o1 reasoning parameters with effort levels                                                          | Multiple caching layers: in-memory, SQLite, Redis, Momento, semantic caching based on similarity                                                                                             | Comprehensive ecosystem: chains, agents, memory, callbacks, extensive provider support (20+) |
| **DSPy**                        | Signature-based contracts defining input/output types rather than messages: `question -> answer: float`          | Structured output objects automatically parsed from LLM responses based on signature definitions                  | **Mixed**: Provider-agnostic tool usage patterns + automatic prompt optimization for provider-specific tools  | Module-based streaming with automatic prompt generation and response parsing                               | **Systematic optimization** - automated reasoning chain synthesis through optimizers that improve prompts and few-shot examples based on metrics and datasets | **Implicit optimization caching** - optimized prompts and examples cached across runs, plus provider-specific caching pass-through                                                          | Programming-over-prompting paradigm, automatic optimization, modular composition, metric-driven improvement |
