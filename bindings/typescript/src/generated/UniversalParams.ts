// This file was generated by [ts-rs](https://github.com/Aleph-Alpha/ts-rs). Do not edit this file manually.
import type { ReasoningConfig } from "./ReasoningConfig";
import type { ResponseFormatConfig } from "./ResponseFormatConfig";
import type { TokenBudget } from "./TokenBudget";
import type { ToolChoiceConfig } from "./ToolChoiceConfig";
import type { UniversalTool } from "./UniversalTool";

/**
 * Common request parameters across providers.
 *
 * Uses canonical names - adapters handle mapping to provider-specific names.
 * Provider-specific fields without canonical mappings are stored in `extras`.
 */
export type UniversalParams = { 
/**
 * Controls randomness: 0 = deterministic, 2 = maximum randomness.
 *
 * **Providers:** OpenAI, Anthropic, Google (`generationConfig.temperature`), Bedrock (`inferenceConfig.temperature`)
 */
temperature: number | null, 
/**
 * Nucleus sampling: only consider tokens with cumulative probability â‰¤ top_p.
 *
 * **Providers:** OpenAI, Anthropic, Google (`generationConfig.topP`), Bedrock (`inferenceConfig.topP`)
 */
top_p: number | null, 
/**
 * Only sample from the top K most likely tokens.
 *
 * **Providers:** Anthropic, Google (`generationConfig.topK`)
 */
top_k: bigint | null, 
/**
 * Random seed for deterministic generation.
 *
 * **Providers:** OpenAI
 */
seed: bigint | null, 
/**
 * Penalize tokens based on whether they've appeared at all (-2.0 to 2.0).
 *
 * **Providers:** OpenAI
 */
presence_penalty: number | null, 
/**
 * Penalize tokens based on how often they've appeared (-2.0 to 2.0).
 *
 * **Providers:** OpenAI
 */
frequency_penalty: number | null, 
/**
 * Generation token budget.
 *
 * **Providers:** OpenAI (`max_tokens`/`max_completion_tokens`/`max_output_tokens`),
 * Anthropic (`max_tokens`), Google (`generationConfig.maxOutputTokens`),
 * Bedrock (`inferenceConfig.maxTokens`) all map to `OutputTokens`.
 */
token_budget: TokenBudget | null, 
/**
 * Sequences that stop generation when encountered.
 *
 * **Providers:** OpenAI, Anthropic (`stop_sequences`), Google (`generationConfig.stopSequences`), Bedrock (`inferenceConfig.stopSequences`)
 */
stop: Array<string> | null, 
/**
 * Return log probabilities of output tokens.
 *
 * **Providers:** OpenAI
 */
logprobs: boolean | null, 
/**
 * Number of most likely tokens to return log probabilities for (0-20).
 *
 * **Providers:** OpenAI
 */
top_logprobs: bigint | null, 
/**
 * Tool/function definitions the model can call.
 *
 * **Providers:** OpenAI, Anthropic, Google (`tools[].functionDeclarations`), Bedrock (`toolConfig.tools[].toolSpec`)
 */
tools: Array<UniversalTool> | null, 
/**
 * How the model should choose which tool to call.
 *
 * **Providers:** OpenAI, Anthropic
 */
tool_choice: ToolChoiceConfig | null, 
/**
 * Allow multiple tool calls in a single response.
 *
 * **Providers:** OpenAI, Anthropic (`tool_choice.disable_parallel_tool_use`)
 */
parallel_tool_calls: boolean | null, 
/**
 * Constrain output format (text, JSON, or JSON schema).
 *
 * **Providers:** OpenAI, Anthropic (`output_format`)
 */
response_format: ResponseFormatConfig | null, 
/**
 * Enable extended thinking / chain-of-thought reasoning.
 *
 * **Providers:** OpenAI (`reasoning_effort`), Anthropic (`thinking`), Google (`generationConfig.thinkingConfig`), Bedrock (`additionalModelRequestFields.thinking`)
 */
reasoning: ReasoningConfig | null, 
/**
 * Key-value metadata attached to the request.
 *
 * **Providers:** OpenAI, Anthropic (only `user_id`)
 */
metadata: Record<string, unknown> | null, 
/**
 * Store the completion for later use in fine-tuning or evals.
 *
 * **Providers:** OpenAI
 */
store: boolean | null, 
/**
 * Request priority tier (e.g., "auto", "default").
 *
 * **Providers:** OpenAI, Anthropic
 */
service_tier: string | null, 
/**
 * Stream the response as server-sent events.
 *
 * **Providers:** OpenAI, Anthropic
 */
stream: boolean | null, };
