// Generated OpenAI types using quicktype
// Essential types for Elmir OpenAI integration
#![allow(clippy::large_enum_variant)]
#![allow(clippy::doc_lazy_continuation)]

// Example code that deserializes and serializes the model.
// extern crate serde;
// #[macro_use]
// extern crate serde_derive;
// extern crate serde_json;
//
// use generated_module::openai_schemas;
//
// fn main() {
//     let json = r#"{"answer": 42}"#;
//     let model: openai_schemas = serde_json::from_str(&json).unwrap();
// }

use serde::{Deserialize, Serialize};
use std::collections::HashMap;

/// Represents a chat completion response returned by model, based on the provided input.
///
/// Represents a streamed chunk of a chat completion response returned
/// by the model, based on the provided input.
/// [Learn more](https://platform.openai.com/docs/guides/streaming-responses).
///
///
/// Developer-provided instructions that the model should follow, regardless of
/// messages sent by the user. With o1 models and newer, `developer` messages
/// replace the previous `system` messages.
///
///
/// Developer-provided instructions that the model should follow, regardless of
/// messages sent by the user. With o1 models and newer, use `developer` messages
/// for this purpose instead.
///
///
/// Messages sent by an end user, containing prompts or additional context
/// information.
///
///
/// Messages sent by the model in response to user messages.
///
///
/// A chat completion message generated by the model.
///
/// A function tool that can be used to generate a response.
///
///
/// Usage statistics for the completion request.
///
/// An optional field that will only be present when you set
/// `stream_options: {"include_usage": true}` in your request. When present, it
/// contains a null value **except for the last chunk** which contains the
/// token usage statistics for the entire request.
///
/// **NOTE:** If the stream is interrupted or cancelled, you may not
/// receive the final usage chunk which contains the total token usage for
/// the request.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct OpenaiSchemas {
    pub metadata: Option<HashMap<String, String>>,
    /// Used by OpenAI to cache responses for similar requests to optimize your cache hit rates.
    /// Replaces the `user` field. [Learn
    /// more](https://platform.openai.com/docs/guides/prompt-caching).
    pub prompt_cache_key: Option<String>,
    /// A stable identifier used to help detect users of your application that may be violating
    /// OpenAI's usage policies.
    /// The IDs should be a string that uniquely identifies each user. We recommend hashing their
    /// username or email address, in order to avoid sending us any identifying information.
    /// [Learn
    /// more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).
    pub safety_identifier: Option<String>,
    pub service_tier: Option<ServiceTier>,
    /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the
    /// output more random, while lower values like 0.2 will make it more focused and
    /// deterministic.
    /// We generally recommend altering this or `top_p` but not both.
    pub temperature: Option<f64>,
    /// An integer between 0 and 20 specifying the number of most likely tokens to
    /// return at each token position, each with an associated log probability.
    ///
    ///
    /// An integer between 0 and 20 specifying the number of most likely tokens to
    /// return at each token position, each with an associated log probability.
    /// `logprobs` must be set to `true` if this parameter is used.
    pub top_logprobs: Option<i64>,
    /// An alternative to sampling with temperature, called nucleus sampling,
    /// where the model considers the results of the tokens with top_p probability
    /// mass. So 0.1 means only the tokens comprising the top 10% probability mass
    /// are considered.
    ///
    /// We generally recommend altering this or `temperature` but not both.
    pub top_p: Option<f64>,
    /// This field is being replaced by `safety_identifier` and `prompt_cache_key`. Use
    /// `prompt_cache_key` instead to maintain caching optimizations.
    /// A stable identifier for your end-users.
    /// Used to boost cache hit rates by better bucketing similar requests and  to help OpenAI
    /// detect and prevent abuse. [Learn
    /// more](https://platform.openai.com/docs/guides/safety-best-practices#safety-identifiers).
    pub user: Option<String>,
    /// Parameters for audio output. Required when audio output is requested with
    /// `modalities: ["audio"]`. [Learn more](https://platform.openai.com/docs/guides/audio).
    ///
    ///
    /// Data about a previous audio response from the model.
    /// [Learn more](https://platform.openai.com/docs/guides/audio).
    ///
    ///
    /// If the audio output modality is requested, this object contains data
    /// about the audio response from the model. [Learn
    /// more](https://platform.openai.com/docs/guides/audio).
    pub audio: Option<OpenaiSchemasAudio>,
    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on
    /// their existing frequency in the text so far, decreasing the model's
    /// likelihood to repeat the same line verbatim.
    pub frequency_penalty: Option<f64>,
    /// Deprecated in favor of `tool_choice`.
    ///
    /// Controls which (if any) function is called by the model.
    ///
    /// `none` means the model will not call a function and instead generates a
    /// message.
    ///
    /// `auto` means the model can pick between generating a message or calling a
    /// function.
    ///
    /// Specifying a particular function via `{"name": "my_function"}` forces the
    /// model to call that function.
    ///
    /// `none` is the default when no functions are present. `auto` is the default
    /// if functions are present.
    ///
    ///
    /// Deprecated and replaced by `tool_calls`. The name and arguments of a function that should
    /// be called, as generated by the model.
    pub function_call: Option<FunctionCallUnion>,
    /// Deprecated in favor of `tools`.
    ///
    /// A list of functions the model may generate JSON inputs for.
    pub functions: Option<Vec<ChatCompletionFunctions>>,
    /// Modify the likelihood of specified tokens appearing in the completion.
    ///
    /// Accepts a JSON object that maps tokens (specified by their token ID in the
    /// tokenizer) to an associated bias value from -100 to 100. Mathematically,
    /// the bias is added to the logits generated by the model prior to sampling.
    /// The exact effect will vary per model, but values between -1 and 1 should
    /// decrease or increase likelihood of selection; values like -100 or 100
    /// should result in a ban or exclusive selection of the relevant token.
    pub logit_bias: Option<HashMap<String, i64>>,
    /// Whether to return log probabilities of the output tokens or not. If true,
    /// returns the log probabilities of each output token returned in the
    /// `content` of `message`.
    pub logprobs: Option<bool>,
    /// An upper bound for the number of tokens that can be generated for a completion, including
    /// visible output tokens and [reasoning
    /// tokens](https://platform.openai.com/docs/guides/reasoning).
    pub max_completion_tokens: Option<i64>,
    /// The maximum number of [tokens](/tokenizer) that can be generated in the
    /// chat completion. This value can be used to control
    /// [costs](https://openai.com/api/pricing/) for text generated via API.
    ///
    /// This value is now deprecated in favor of `max_completion_tokens`, and is
    /// not compatible with [o-series models](https://platform.openai.com/docs/guides/reasoning).
    pub max_tokens: Option<i64>,
    /// A list of messages comprising the conversation so far. Depending on the
    /// [model](https://platform.openai.com/docs/models) you use, different message types
    /// (modalities) are
    /// supported, like [text](https://platform.openai.com/docs/guides/text-generation),
    /// [images](https://platform.openai.com/docs/guides/vision), and
    /// [audio](https://platform.openai.com/docs/guides/audio).
    pub messages: Option<Vec<ChatCompletionRequestMessage>>,
    pub modalities: Option<Vec<ResponseModality>>,
    /// Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI
    /// offers a wide range of models with different capabilities, performance
    /// characteristics, and price points. Refer to the [model
    /// guide](https://platform.openai.com/docs/models)
    /// to browse and compare available models.
    ///
    ///
    /// The model used for the chat completion.
    ///
    /// The model to generate the completion.
    pub model: Option<String>,
    /// How many chat completion choices to generate for each input message. Note that you will
    /// be charged based on the number of generated tokens across all of the choices. Keep `n` as
    /// `1` to minimize costs.
    pub n: Option<i64>,
    pub parallel_tool_calls: Option<bool>,
    /// Configuration for a [Predicted
    /// Output](https://platform.openai.com/docs/guides/predicted-outputs),
    /// which can greatly improve response times when large parts of the model
    /// response are known ahead of time. This is most common when you are
    /// regenerating a file with only minor changes to most of the content.
    pub prediction: Option<StaticContent>,
    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on
    /// whether they appear in the text so far, increasing the model's likelihood
    /// to talk about new topics.
    pub presence_penalty: Option<f64>,
    pub reasoning_effort: Option<ReasoningEffort>,
    /// An object specifying the format that the model must output.
    ///
    /// Setting to `{ "type": "json_schema", "json_schema": {...} }` enables
    /// Structured Outputs which ensures the model will match your supplied JSON
    /// schema. Learn more in the [Structured Outputs
    /// guide](https://platform.openai.com/docs/guides/structured-outputs).
    ///
    /// Setting to `{ "type": "json_object" }` enables the older JSON mode, which
    /// ensures the message the model generates is valid JSON. Using `json_schema`
    /// is preferred for models that support it.
    pub response_format: Option<Text>,
    /// This feature is in Beta.
    /// If specified, our system will make a best effort to sample deterministically, such that
    /// repeated requests with the same `seed` and parameters should return the same result.
    /// Determinism is not guaranteed, and you should refer to the `system_fingerprint` response
    /// parameter to monitor changes in the backend.
    pub seed: Option<i64>,
    pub stop: Option<StopConfiguration>,
    /// Whether or not to store the output of this chat completion request for
    /// use in our [model distillation](https://platform.openai.com/docs/guides/distillation) or
    /// [evals](https://platform.openai.com/docs/guides/evals) products.
    ///
    /// Supports text and image inputs. Note: image inputs over 8MB will be dropped.
    pub store: Option<bool>,
    /// If set to true, the model response data will be streamed to the client
    /// as it is generated using [server-sent
    /// events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).
    /// See the [Streaming section
    /// below](https://platform.openai.com/docs/api-reference/chat/streaming)
    /// for more information, along with the [streaming
    /// responses](https://platform.openai.com/docs/guides/streaming-responses)
    /// guide for more information on how to handle the streaming events.
    pub stream: Option<bool>,
    pub stream_options: Option<ChatCompletionStreamOptions>,
    pub tool_choice: Option<ChatCompletionToolChoiceOption>,
    /// A list of tools the model may call. You can provide either
    /// [custom tools](https://platform.openai.com/docs/guides/function-calling#custom-tools) or
    /// [function tools](https://platform.openai.com/docs/guides/function-calling).
    pub tools: Option<Vec<Tool>>,
    pub verbosity: Option<Verbosity>,
    /// This tool searches the web for relevant results to use in a response.
    /// Learn more about the [web search
    /// tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).
    pub web_search_options: Option<WebSearch>,
    /// A list of chat completion choices. Can be more than one if `n` is greater than 1.
    ///
    /// A list of chat completion choices. Can contain more than one elements if `n` is greater
    /// than 1. Can also be empty for the
    /// last chunk if you set `stream_options: {"include_usage": true}`.
    pub choices: Option<Vec<Choice>>,
    /// The Unix timestamp (in seconds) of when the chat completion was created.
    ///
    /// The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has
    /// the same timestamp.
    pub created: Option<i64>,
    /// A unique identifier for the chat completion.
    ///
    /// A unique identifier for the chat completion. Each chunk has the same ID.
    pub id: Option<String>,
    /// The object type, which is always `chat.completion`.
    ///
    /// The object type, which is always `chat.completion.chunk`.
    pub object: Option<Object>,
    /// This fingerprint represents the backend configuration that the model runs with.
    ///
    /// Can be used in conjunction with the `seed` request parameter to understand when backend
    /// changes have been made that might impact determinism.
    ///
    ///
    /// This fingerprint represents the backend configuration that the model runs with.
    /// Can be used in conjunction with the `seed` request parameter to understand when backend
    /// changes have been made that might impact determinism.
    pub system_fingerprint: Option<String>,
    /// An optional field that will only be present when you set
    /// `stream_options: {"include_usage": true}` in your request. When present, it
    /// contains a null value **except for the last chunk** which contains the
    /// token usage statistics for the entire request.
    ///
    /// **NOTE:** If the stream is interrupted or cancelled, you may not
    /// receive the final usage chunk which contains the total token usage for
    /// the request.
    pub usage: Option<CompletionUsage>,
    /// The contents of the developer message.
    ///
    /// The contents of the system message.
    ///
    /// The contents of the user message.
    ///
    ///
    /// The contents of the assistant message. Required unless `tool_calls` or `function_call` is
    /// specified.
    ///
    ///
    /// The contents of the tool message.
    ///
    /// The contents of the function message.
    ///
    /// The contents of the message.
    pub content: Option<OpenaiSchemasContent>,
    /// An optional name for the participant. Provides the model information to differentiate
    /// between participants of the same role.
    ///
    /// The name of the function to call.
    pub name: Option<String>,
    /// The role of the messages author, in this case `developer`.
    ///
    /// The role of the messages author, in this case `system`.
    ///
    /// The role of the messages author, in this case `user`.
    ///
    /// The role of the messages author, in this case `assistant`.
    ///
    /// The role of the messages author, in this case `tool`.
    ///
    /// The role of the messages author, in this case `function`.
    ///
    /// The role of the author of this message.
    pub role: Option<ChatCompletionRequestMessageRole>,
    /// The refusal message by the assistant.
    ///
    /// The refusal message generated by the model.
    pub refusal: Option<String>,
    pub tool_calls: Option<Vec<ToolCall>>,
    /// Tool call that this message is responding to.
    pub tool_call_id: Option<String>,
    /// Annotations for the message, when applicable, as when using the
    /// [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).
    pub annotations: Option<Vec<Annotation>>,
    pub function: Option<FunctionObject>,
    /// The type of the tool. Currently, only `function` is supported.
    #[serde(rename = "type")]
    pub openai_schemas_type: Option<OpenaiSchemasType>,
    /// Number of tokens in the generated completion.
    pub completion_tokens: Option<i64>,
    /// Breakdown of tokens used in a completion.
    pub completion_tokens_details: Option<CompletionTokensDetails>,
    /// Number of tokens in the prompt.
    pub prompt_tokens: Option<i64>,
    /// Breakdown of tokens used in the prompt.
    pub prompt_tokens_details: Option<PromptTokensDetails>,
    /// Total number of tokens used in the request (prompt + completion).
    pub total_tokens: Option<i64>,
}

/// A URL citation when using web search.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct Annotation {
    /// The type of the URL citation. Always `url_citation`.
    #[serde(rename = "type")]
    pub annotation_type: AnnotationType,
    /// A URL citation when using web search.
    pub url_citation: UrlCitation,
}

/// The type of the URL citation. Always `url_citation`.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum AnnotationType {
    #[serde(rename = "url_citation")]
    UrlCitation,
}

/// A URL citation when using web search.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct UrlCitation {
    /// The index of the last character of the URL citation in the message.
    pub end_index: i64,
    /// The index of the first character of the URL citation in the message.
    pub start_index: i64,
    /// The title of the web resource.
    pub title: String,
    /// The URL of the web resource.
    pub url: String,
}

/// Parameters for audio output. Required when audio output is requested with
/// `modalities: ["audio"]`. [Learn more](https://platform.openai.com/docs/guides/audio).
///
///
/// Data about a previous audio response from the model.
/// [Learn more](https://platform.openai.com/docs/guides/audio).
///
///
/// If the audio output modality is requested, this object contains data
/// about the audio response from the model. [Learn
/// more](https://platform.openai.com/docs/guides/audio).
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct OpenaiSchemasAudio {
    /// Specifies the output audio format. Must be one of `wav`, `mp3`, `flac`,
    /// `opus`, or `pcm16`.
    pub format: Option<AudioFormat>,
    /// The voice the model uses to respond. Supported voices are
    /// `alloy`, `ash`, `ballad`, `coral`, `echo`, `fable`, `nova`, `onyx`, `sage`, and `shimmer`.
    pub voice: Option<String>,
    /// Unique identifier for a previous audio response from the model.
    ///
    ///
    /// Unique identifier for this audio response.
    pub id: Option<String>,
    /// Base64 encoded audio bytes generated by the model, in the format
    /// specified in the request.
    pub data: Option<String>,
    /// The Unix timestamp (in seconds) for when this audio response will
    /// no longer be accessible on the server for use in multi-turn
    /// conversations.
    pub expires_at: Option<i64>,
    /// Transcript of the audio generated by the model.
    pub transcript: Option<String>,
}

/// Specifies the output audio format. Must be one of `wav`, `mp3`, `flac`,
/// `opus`, or `pcm16`.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum AudioFormat {
    Aac,
    Flac,
    Mp3,
    Opus,
    Pcm16,
    Wav,
}

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct Choice {
    /// The reason the model stopped generating tokens. This will be `stop` if the model hit a
    /// natural stop point or a provided stop sequence,
    /// `length` if the maximum number of tokens specified in the request was reached,
    /// `content_filter` if content was omitted due to a flag from our content filters,
    /// `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model
    /// called a function.
    pub finish_reason: FinishReason,
    /// The index of the choice in the list of choices.
    pub index: i64,
    /// Log probability information for the choice.
    pub logprobs: Option<Logprobs>,
    pub message: Option<ChatCompletionResponseMessage>,
    pub delta: Option<ChatCompletionStreamResponseDelta>,
}

/// A chat completion delta generated by streamed model responses.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct ChatCompletionStreamResponseDelta {
    /// The contents of the chunk message.
    pub content: Option<String>,
    /// Deprecated and replaced by `tool_calls`. The name and arguments of a function that should
    /// be called, as generated by the model.
    pub function_call: Option<DeltaFunctionCall>,
    /// The refusal message generated by the model.
    pub refusal: Option<String>,
    /// The role of the author of this message.
    pub role: Option<DeltaRole>,
    pub tool_calls: Option<Vec<ChatCompletionMessageToolCallChunk>>,
}

/// Deprecated and replaced by `tool_calls`. The name and arguments of a function that should
/// be called, as generated by the model.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct DeltaFunctionCall {
    /// The arguments to call the function with, as generated by the model in JSON format. Note
    /// that the model does not always generate valid JSON, and may hallucinate parameters not
    /// defined by your function schema. Validate the arguments in your code before calling your
    /// function.
    pub arguments: Option<String>,
    /// The name of the function to call.
    pub name: Option<String>,
}

/// The role of the author of this message.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum DeltaRole {
    Assistant,
    Developer,
    System,
    Tool,
    User,
}

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct ChatCompletionMessageToolCallChunk {
    pub function: Option<PurpleFunction>,
    /// The ID of the tool call.
    pub id: Option<String>,
    pub index: i64,
    /// The type of the tool. Currently, only `function` is supported.
    #[serde(rename = "type")]
    pub chat_completion_message_tool_call_chunk_type: Option<OpenaiSchemasType>,
}

/// The type of the tool. Currently, only `function` is supported.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum OpenaiSchemasType {
    Function,
}

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct PurpleFunction {
    /// The arguments to call the function with, as generated by the model in JSON format. Note
    /// that the model does not always generate valid JSON, and may hallucinate parameters not
    /// defined by your function schema. Validate the arguments in your code before calling your
    /// function.
    pub arguments: Option<String>,
    /// The name of the function to call.
    pub name: Option<String>,
}

/// The reason the model stopped generating tokens. This will be `stop` if the model hit a
/// natural stop point or a provided stop sequence,
/// `length` if the maximum number of tokens specified in the request was reached,
/// `content_filter` if content was omitted due to a flag from our content filters,
/// `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model
/// called a function.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum FinishReason {
    #[serde(rename = "content_filter")]
    ContentFilter,
    #[serde(rename = "function_call")]
    FunctionCall,
    Length,
    Stop,
    #[serde(rename = "tool_calls")]
    ToolCalls,
}

/// Log probability information for the choice.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct Logprobs {
    /// A list of message content tokens with log probability information.
    pub content: Vec<ChatCompletionTokenLogprob>,
    /// A list of message refusal tokens with log probability information.
    pub refusal: Vec<ChatCompletionTokenLogprob>,
}

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct ChatCompletionTokenLogprob {
    /// A list of integers representing the UTF-8 bytes representation of the token. Useful in
    /// instances where characters are represented by multiple tokens and their byte
    /// representations must be combined to generate the correct text representation. Can be
    /// `null` if there is no bytes representation for the token.
    pub bytes: Vec<i64>,
    /// The log probability of this token, if it is within the top 20 most likely tokens.
    /// Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
    pub logprob: f64,
    /// The token.
    pub token: String,
    /// List of the most likely tokens and their log probability, at this token position. In rare
    /// cases, there may be fewer than the number of requested `top_logprobs` returned.
    pub top_logprobs: Vec<TopLogprob>,
}

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct TopLogprob {
    /// A list of integers representing the UTF-8 bytes representation of the token. Useful in
    /// instances where characters are represented by multiple tokens and their byte
    /// representations must be combined to generate the correct text representation. Can be
    /// `null` if there is no bytes representation for the token.
    pub bytes: Vec<i64>,
    /// The log probability of this token, if it is within the top 20 most likely tokens.
    /// Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
    pub logprob: f64,
    /// The token.
    pub token: String,
}

/// A chat completion message generated by the model.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct ChatCompletionResponseMessage {
    /// Annotations for the message, when applicable, as when using the
    /// [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).
    pub annotations: Option<Vec<Annotation>>,
    /// If the audio output modality is requested, this object contains data
    /// about the audio response from the model. [Learn
    /// more](https://platform.openai.com/docs/guides/audio).
    pub audio: Option<MessageAudio>,
    /// The contents of the message.
    pub content: String,
    /// Deprecated and replaced by `tool_calls`. The name and arguments of a function that should
    /// be called, as generated by the model.
    pub function_call: Option<MessageFunctionCall>,
    /// The refusal message generated by the model.
    pub refusal: String,
    /// The role of the author of this message.
    pub role: MessageRole,
    pub tool_calls: Option<Vec<ToolCall>>,
}

/// If the audio output modality is requested, this object contains data
/// about the audio response from the model. [Learn
/// more](https://platform.openai.com/docs/guides/audio).
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct MessageAudio {
    /// Base64 encoded audio bytes generated by the model, in the format
    /// specified in the request.
    pub data: String,
    /// The Unix timestamp (in seconds) for when this audio response will
    /// no longer be accessible on the server for use in multi-turn
    /// conversations.
    pub expires_at: i64,
    /// Unique identifier for this audio response.
    pub id: String,
    /// Transcript of the audio generated by the model.
    pub transcript: String,
}

/// Deprecated and replaced by `tool_calls`. The name and arguments of a function that should
/// be called, as generated by the model.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct MessageFunctionCall {
    /// The arguments to call the function with, as generated by the model in JSON format. Note
    /// that the model does not always generate valid JSON, and may hallucinate parameters not
    /// defined by your function schema. Validate the arguments in your code before calling your
    /// function.
    pub arguments: String,
    /// The name of the function to call.
    pub name: String,
}

/// The role of the author of this message.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum MessageRole {
    Assistant,
}

/// The tool calls generated by the model, such as function calls.
///
/// A call to a function tool created by the model.
///
///
/// A call to a custom tool created by the model.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct ToolCall {
    /// The function that the model called.
    pub function: Option<FluffyFunction>,
    /// The ID of the tool call.
    pub id: String,
    /// The type of the tool. Currently, only `function` is supported.
    ///
    /// The type of the tool. Always `custom`.
    #[serde(rename = "type")]
    pub tool_call_type: ToolType,
    /// The custom tool that the model called.
    pub custom: Option<ToolCallCustom>,
}

/// The custom tool that the model called.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct ToolCallCustom {
    /// The input for the custom tool call generated by the model.
    pub input: String,
    /// The name of the custom tool to call.
    pub name: String,
}

/// The function that the model called.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct FluffyFunction {
    /// The arguments to call the function with, as generated by the model in JSON format. Note
    /// that the model does not always generate valid JSON, and may hallucinate parameters not
    /// defined by your function schema. Validate the arguments in your code before calling your
    /// function.
    pub arguments: String,
    /// The name of the function to call.
    pub name: String,
}

/// The type of the tool. Currently, only `function` is supported.
///
/// The type of the tool. Always `custom`.
///
/// The type of the custom tool. Always `custom`.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ToolType {
    Custom,
    Function,
}

/// Breakdown of tokens used in a completion.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct CompletionTokensDetails {
    /// When using Predicted Outputs, the number of tokens in the
    /// prediction that appeared in the completion.
    pub accepted_prediction_tokens: Option<i64>,
    /// Audio input tokens generated by the model.
    pub audio_tokens: Option<i64>,
    /// Tokens generated by the model for reasoning.
    pub reasoning_tokens: Option<i64>,
    /// When using Predicted Outputs, the number of tokens in the
    /// prediction that did not appear in the completion. However, like
    /// reasoning tokens, these tokens are still counted in the total
    /// completion tokens for purposes of billing, output, and context window
    /// limits.
    pub rejected_prediction_tokens: Option<i64>,
}

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(untagged)]
pub enum OpenaiSchemasContent {
    ChatCompletionRequestMessageContentPartArray(Vec<ChatCompletionRequestMessageContentPart>),
    String(String),
}

/// An array of content parts with a defined type. For developer messages, only type `text`
/// is supported.
///
/// Learn about [text inputs](https://platform.openai.com/docs/guides/text-generation).
///
///
/// An array of content parts with a defined type. Supported options differ based on the
/// [model](https://platform.openai.com/docs/models) being used to generate the response. Can
/// contain text inputs.
///
/// An array of content parts with a defined type. For system messages, only type `text` is
/// supported.
///
/// An array of content parts with a defined type. For tool messages, only type `text` is
/// supported.
///
/// An array of content parts with a defined type. Supported options differ based on the
/// [model](https://platform.openai.com/docs/models) being used to generate the response. Can
/// contain text, image, or audio inputs.
///
/// Learn about [image inputs](https://platform.openai.com/docs/guides/vision).
///
///
/// Learn about [audio inputs](https://platform.openai.com/docs/guides/audio).
///
///
/// Learn about [file inputs](https://platform.openai.com/docs/guides/text) for text
/// generation.
///
///
/// An array of content parts with a defined type. Can be one or more of type `text`, or
/// exactly one of type `refusal`.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct ChatCompletionRequestMessageContentPart {
    /// The text content.
    pub text: Option<String>,
    /// The type of the content part.
    ///
    /// The type of the content part. Always `input_audio`.
    ///
    /// The type of the content part. Always `file`.
    #[serde(rename = "type")]
    pub chat_completion_request_message_content_part_type: PurpleType,
    pub image_url: Option<ImageUrl>,
    pub input_audio: Option<InputAudio>,
    pub file: Option<File>,
    /// The refusal message generated by the model.
    pub refusal: Option<String>,
}

/// The type of the content part.
///
/// The type of the content part. Always `input_audio`.
///
/// The type of the content part. Always `file`.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum PurpleType {
    File,
    #[serde(rename = "image_url")]
    ImageUrl,
    #[serde(rename = "input_audio")]
    InputAudio,
    Refusal,
    Text,
}

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct File {
    /// The base64 encoded file data, used when passing the file to the model
    /// as a string.
    pub file_data: Option<String>,
    /// The ID of an uploaded file to use as input.
    pub file_id: Option<String>,
    /// The name of the file, used when passing the file to the model as a
    /// string.
    pub filename: Option<String>,
}

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct ImageUrl {
    /// Specifies the detail level of the image. Learn more in the [Vision
    /// guide](https://platform.openai.com/docs/guides/vision#low-or-high-fidelity-image-understanding).
    pub detail: Option<Detail>,
    /// Either a URL of the image or the base64 encoded image data.
    pub url: String,
}

/// Specifies the detail level of the image. Learn more in the [Vision
/// guide](https://platform.openai.com/docs/guides/vision#low-or-high-fidelity-image-understanding).
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum Detail {
    Auto,
    High,
    Low,
}

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct InputAudio {
    /// Base64 encoded audio data.
    pub data: String,
    /// The format of the encoded audio data. Currently supports "wav" and "mp3".
    pub format: InputAudioFormat,
}

/// The format of the encoded audio data. Currently supports "wav" and "mp3".
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum InputAudioFormat {
    Mp3,
    Wav,
}

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct FunctionObject {
    /// A description of what the function does, used by the model to choose when and how to call
    /// the function.
    pub description: Option<String>,
    /// The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and
    /// dashes, with a maximum length of 64.
    pub name: String,
    pub parameters: Option<HashMap<String, Option<serde_json::Value>>>,
    /// Whether to enable strict schema adherence when generating the function call. If set to
    /// true, the model will follow the exact schema defined in the `parameters` field. Only a
    /// subset of JSON Schema is supported when `strict` is `true`. Learn more about Structured
    /// Outputs in the [function calling
    /// guide](https://platform.openai.com/docs/guides/function-calling).
    pub strict: Option<bool>,
}

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(untagged)]
pub enum FunctionCallUnion {
    ChatCompletionFunctionCallOption(ChatCompletionFunctionCallOption),
    Enum(FunctionCallMode),
}

/// Specifying a particular function via `{"name": "my_function"}` forces the model to call
/// that function.
///
///
/// Deprecated and replaced by `tool_calls`. The name and arguments of a function that should
/// be called, as generated by the model.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct ChatCompletionFunctionCallOption {
    /// The name of the function to call.
    pub name: String,
    /// The arguments to call the function with, as generated by the model in JSON format. Note
    /// that the model does not always generate valid JSON, and may hallucinate parameters not
    /// defined by your function schema. Validate the arguments in your code before calling your
    /// function.
    pub arguments: Option<String>,
}

/// `none` means the model will not call a function and instead generates a message. `auto`
/// means the model can pick between generating a message or calling a function.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum FunctionCallMode {
    Auto,
    None,
}

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct ChatCompletionFunctions {
    /// A description of what the function does, used by the model to choose when and how to call
    /// the function.
    pub description: Option<String>,
    /// The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and
    /// dashes, with a maximum length of 64.
    pub name: String,
    pub parameters: Option<HashMap<String, Option<serde_json::Value>>>,
}

/// Developer-provided instructions that the model should follow, regardless of
/// messages sent by the user. With o1 models and newer, `developer` messages
/// replace the previous `system` messages.
///
///
/// Developer-provided instructions that the model should follow, regardless of
/// messages sent by the user. With o1 models and newer, use `developer` messages
/// for this purpose instead.
///
///
/// Messages sent by an end user, containing prompts or additional context
/// information.
///
///
/// Messages sent by the model in response to user messages.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct ChatCompletionRequestMessage {
    /// The contents of the developer message.
    ///
    /// The contents of the system message.
    ///
    /// The contents of the user message.
    ///
    ///
    /// The contents of the assistant message. Required unless `tool_calls` or `function_call` is
    /// specified.
    ///
    ///
    /// The contents of the tool message.
    ///
    /// The contents of the function message.
    pub content: Option<OpenaiSchemasContent>,
    /// An optional name for the participant. Provides the model information to differentiate
    /// between participants of the same role.
    ///
    /// The name of the function to call.
    pub name: Option<String>,
    /// The role of the messages author, in this case `developer`.
    ///
    /// The role of the messages author, in this case `system`.
    ///
    /// The role of the messages author, in this case `user`.
    ///
    /// The role of the messages author, in this case `assistant`.
    ///
    /// The role of the messages author, in this case `tool`.
    ///
    /// The role of the messages author, in this case `function`.
    pub role: ChatCompletionRequestMessageRole,
    /// Data about a previous audio response from the model.
    /// [Learn more](https://platform.openai.com/docs/guides/audio).
    pub audio: Option<ChatCompletionRequestMessageAudio>,
    /// Deprecated and replaced by `tool_calls`. The name and arguments of a function that should
    /// be called, as generated by the model.
    pub function_call: Option<ChatCompletionRequestMessageFunctionCall>,
    /// The refusal message by the assistant.
    pub refusal: Option<String>,
    pub tool_calls: Option<Vec<ToolCall>>,
    /// Tool call that this message is responding to.
    pub tool_call_id: Option<String>,
}

/// Data about a previous audio response from the model.
/// [Learn more](https://platform.openai.com/docs/guides/audio).
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct ChatCompletionRequestMessageAudio {
    /// Unique identifier for a previous audio response from the model.
    pub id: String,
}

/// Deprecated and replaced by `tool_calls`. The name and arguments of a function that should
/// be called, as generated by the model.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct ChatCompletionRequestMessageFunctionCall {
    /// The arguments to call the function with, as generated by the model in JSON format. Note
    /// that the model does not always generate valid JSON, and may hallucinate parameters not
    /// defined by your function schema. Validate the arguments in your code before calling your
    /// function.
    pub arguments: String,
    /// The name of the function to call.
    pub name: String,
}

/// The role of the messages author, in this case `developer`.
///
/// The role of the messages author, in this case `system`.
///
/// The role of the messages author, in this case `user`.
///
/// The role of the messages author, in this case `assistant`.
///
/// The role of the messages author, in this case `tool`.
///
/// The role of the messages author, in this case `function`.
///
/// The role of the author of this message.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ChatCompletionRequestMessageRole {
    Assistant,
    Developer,
    Function,
    System,
    Tool,
    User,
}

/// Output types that you would like the model to generate.
/// Most models are capable of generating text, which is the default:
///
/// `["text"]`
///
/// The `gpt-4o-audio-preview` model can also be used to
/// [generate audio](https://platform.openai.com/docs/guides/audio). To request that this
/// model generate
/// both text and audio responses, you can use:
///
/// `["text", "audio"]`
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ResponseModality {
    Audio,
    Text,
}

/// The object type, which is always `chat.completion`.
///
/// The object type, which is always `chat.completion.chunk`.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub enum Object {
    #[serde(rename = "chat.completion")]
    ChatCompletion,
    #[serde(rename = "chat.completion.chunk")]
    ChatCompletionChunk,
}

/// Configuration for a [Predicted
/// Output](https://platform.openai.com/docs/guides/predicted-outputs),
/// which can greatly improve response times when large parts of the model
/// response are known ahead of time. This is most common when you are
/// regenerating a file with only minor changes to most of the content.
///
///
/// Static predicted output content, such as the content of a text file that is
/// being regenerated.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct StaticContent {
    /// The content that should be matched when generating a model response.
    /// If generated tokens would match this content, the entire model response
    /// can be returned much more quickly.
    pub content: PredictionContent,
    /// The type of the predicted content you want to provide. This type is
    /// currently always `content`.
    #[serde(rename = "type")]
    pub static_content_type: PredictionType,
}

/// The contents of the system message.
///
/// The contents of the tool message.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(untagged)]
pub enum PredictionContent {
    ContentPartArray(Vec<ContentPart>),
    String(String),
}

/// An array of content parts with a defined type. For developer messages, only type `text`
/// is supported.
///
/// Learn about [text inputs](https://platform.openai.com/docs/guides/text-generation).
///
///
/// An array of content parts with a defined type. Supported options differ based on the
/// [model](https://platform.openai.com/docs/models) being used to generate the response. Can
/// contain text inputs.
///
/// An array of content parts with a defined type. For system messages, only type `text` is
/// supported.
///
/// An array of content parts with a defined type. For tool messages, only type `text` is
/// supported.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct ContentPart {
    /// The text content.
    pub text: String,
    /// The type of the content part.
    #[serde(rename = "type")]
    pub content_part_type: FluffyType,
}

/// The type of the content part.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum FluffyType {
    Text,
}

/// The type of the predicted content you want to provide. This type is
/// currently always `content`.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum PredictionType {
    Content,
}

/// Breakdown of tokens used in the prompt.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct PromptTokensDetails {
    /// Audio input tokens present in the prompt.
    pub audio_tokens: Option<i64>,
    /// Cached tokens present in the prompt.
    pub cached_tokens: Option<i64>,
}

/// Constrains effort on reasoning for
/// [reasoning models](https://platform.openai.com/docs/guides/reasoning).
/// Currently supported values are `minimal`, `low`, `medium`, and `high`. Reducing
/// reasoning effort can result in faster responses and fewer tokens used
/// on reasoning in a response.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ReasoningEffort {
    High,
    Low,
    Medium,
    Minimal,
}

/// An object specifying the format that the model must output.
///
/// Setting to `{ "type": "json_schema", "json_schema": {...} }` enables
/// Structured Outputs which ensures the model will match your supplied JSON
/// schema. Learn more in the [Structured Outputs
/// guide](https://platform.openai.com/docs/guides/structured-outputs).
///
/// Setting to `{ "type": "json_object" }` enables the older JSON mode, which
/// ensures the message the model generates is valid JSON. Using `json_schema`
/// is preferred for models that support it.
///
///
/// Default response format. Used to generate text responses.
///
///
/// JSON Schema response format. Used to generate structured JSON responses.
/// Learn more about [Structured
/// Outputs](https://platform.openai.com/docs/guides/structured-outputs).
///
///
/// JSON object response format. An older method of generating JSON responses.
/// Using `json_schema` is recommended for models that support it. Note that the
/// model will not generate JSON without a system or user message instructing it
/// to do so.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct Text {
    /// The type of response format being defined. Always `text`.
    ///
    /// The type of response format being defined. Always `json_schema`.
    ///
    /// The type of response format being defined. Always `json_object`.
    #[serde(rename = "type")]
    pub text_type: ResponseFormatType,
    /// Structured Outputs configuration options, including a JSON Schema.
    pub json_schema: Option<JsonSchema>,
}

/// Structured Outputs configuration options, including a JSON Schema.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct JsonSchema {
    /// A description of what the response format is for, used by the model to
    /// determine how to respond in the format.
    pub description: Option<String>,
    /// The name of the response format. Must be a-z, A-Z, 0-9, or contain
    /// underscores and dashes, with a maximum length of 64.
    pub name: String,
    pub schema: Option<HashMap<String, Option<serde_json::Value>>>,
    /// Whether to enable strict schema adherence when generating the output.
    /// If set to true, the model will always follow the exact schema defined
    /// in the `schema` field. Only a subset of JSON Schema is supported when
    /// `strict` is `true`. To learn more, read the [Structured Outputs
    /// guide](https://platform.openai.com/docs/guides/structured-outputs).
    pub strict: Option<bool>,
}

/// The type of response format being defined. Always `text`.
///
/// The type of response format being defined. Always `json_schema`.
///
/// The type of response format being defined. Always `json_object`.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ResponseFormatType {
    #[serde(rename = "json_object")]
    JsonObject,
    #[serde(rename = "json_schema")]
    JsonSchema,
    Text,
}

/// Specifies the processing type used for serving the request.
/// - If set to 'auto', then the request will be processed with the service tier configured
/// in the Project settings. Unless otherwise configured, the Project will use 'default'.
/// - If set to 'default', then the request will be processed with the standard pricing and
/// performance for the selected model.
/// - If set to '[flex](https://platform.openai.com/docs/guides/flex-processing)' or
/// '[priority](https://openai.com/api-priority-processing/)', then the request will be
/// processed with the corresponding service tier.
/// - When not set, the default behavior is 'auto'.
///
/// When the `service_tier` parameter is set, the response body will include the
/// `service_tier` value based on the processing mode actually used to serve the request.
/// This response value may be different from the value set in the parameter.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum ServiceTier {
    Auto,
    Default,
    Flex,
    Priority,
    Scale,
}

/// Not supported with latest reasoning models `o3` and `o4-mini`.
///
/// Up to 4 sequences where the API will stop generating further tokens. The
/// returned text will not contain the stop sequence.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(untagged)]
pub enum StopConfiguration {
    String(String),
    StringArray(Vec<String>),
}

/// Options for streaming response. Only set this when you set `stream: true`.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct ChatCompletionStreamOptions {
    /// When true, stream obfuscation will be enabled. Stream obfuscation adds
    /// random characters to an `obfuscation` field on streaming delta events to
    /// normalize payload sizes as a mitigation to certain side-channel attacks.
    /// These obfuscation fields are included by default, but add a small amount
    /// of overhead to the data stream. You can set `include_obfuscation` to
    /// false to optimize for bandwidth if you trust the network links between
    /// your application and the OpenAI API.
    pub include_obfuscation: Option<bool>,
    /// If set, an additional chunk will be streamed before the `data: [DONE]`
    /// message. The `usage` field on this chunk shows the token usage statistics
    /// for the entire request, and the `choices` field will always be an empty
    /// array.
    ///
    /// All other chunks will also include a `usage` field, but with a null
    /// value. **NOTE:** If the stream is interrupted, you may not receive the
    /// final usage chunk which contains the total token usage for the request.
    pub include_usage: Option<bool>,
}

/// Controls which (if any) tool is called by the model.
/// `none` means the model will not call any tool and instead generates a message.
/// `auto` means the model can pick between generating a message or calling one or more
/// tools.
/// `required` means the model must call one or more tools.
/// Specifying a particular tool via `{"type": "function", "function": {"name":
/// "my_function"}}` forces the model to call that tool.
///
/// `none` is the default when no tools are present. `auto` is the default if tools are
/// present.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(untagged)]
pub enum ChatCompletionToolChoiceOption {
    AllowedTools(AllowedTools),
    Enum(Auto),
}

/// Constrains the tools available to the model to a pre-defined set.
///
///
/// Specifies a tool the model should use. Use to force the model to call a specific
/// function.
///
/// Specifies a tool the model should use. Use to force the model to call a specific custom
/// tool.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct AllowedTools {
    pub allowed_tools: Option<AllowedToolsClass>,
    /// Allowed tool configuration type. Always `allowed_tools`.
    ///
    /// For function calling, the type is always `function`.
    ///
    /// For custom tool calling, the type is always `custom`.
    #[serde(rename = "type")]
    pub allowed_tools_type: AllowedToolsType,
    pub function: Option<AllowedToolsFunction>,
    pub custom: Option<AllowedToolsCustom>,
}

/// Constrains the tools available to the model to a pre-defined set.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct AllowedToolsClass {
    /// Constrains the tools available to the model to a pre-defined set.
    ///
    /// `auto` allows the model to pick from among the allowed tools and generate a
    /// message.
    ///
    /// `required` requires the model to call one or more of the allowed tools.
    pub mode: Mode,
    /// A list of tool definitions that the model should be allowed to call.
    ///
    /// For the Chat Completions API, the list of tool definitions might look like:
    /// ```json
    /// [
    /// { "type": "function", "function": { "name": "get_weather" } },
    /// { "type": "function", "function": { "name": "get_time" } }
    /// ]
    /// ```
    pub tools: Vec<HashMap<String, Option<serde_json::Value>>>,
}

/// Constrains the tools available to the model to a pre-defined set.
///
/// `auto` allows the model to pick from among the allowed tools and generate a
/// message.
///
/// `required` requires the model to call one or more of the allowed tools.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum Mode {
    Auto,
    Required,
}

/// Allowed tool configuration type. Always `allowed_tools`.
///
/// For function calling, the type is always `function`.
///
/// For custom tool calling, the type is always `custom`.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum AllowedToolsType {
    #[serde(rename = "allowed_tools")]
    AllowedTools,
    Custom,
    Function,
}

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct AllowedToolsCustom {
    /// The name of the custom tool to call.
    pub name: String,
}

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct AllowedToolsFunction {
    /// The name of the function to call.
    pub name: String,
}

/// `none` means the model will not call any tool and instead generates a message. `auto`
/// means the model can pick between generating a message or calling one or more tools.
/// `required` means the model must call one or more tools.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum Auto {
    Auto,
    None,
    Required,
}

/// A function tool that can be used to generate a response.
///
///
/// A custom tool that processes input using a specified format.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct Tool {
    pub function: Option<FunctionObject>,
    /// The type of the tool. Currently, only `function` is supported.
    ///
    /// The type of the custom tool. Always `custom`.
    #[serde(rename = "type")]
    pub tool_type: ToolType,
    /// Properties of the custom tool.
    pub custom: Option<CustomToolProperties>,
}

/// Properties of the custom tool.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct CustomToolProperties {
    /// Optional description of the custom tool, used to provide more context.
    pub description: Option<String>,
    /// The input format for the custom tool. Default is unconstrained text.
    pub format: Option<Format>,
    /// The name of the custom tool, used to identify it in tool calls.
    pub name: String,
}

/// The input format for the custom tool. Default is unconstrained text.
///
///
/// Unconstrained free-form text.
///
/// A grammar defined by the user.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct Format {
    /// Unconstrained text format. Always `text`.
    ///
    /// Grammar format. Always `grammar`.
    #[serde(rename = "type")]
    pub format_type: FormatType,
    /// Your chosen grammar.
    pub grammar: Option<GrammarFormat>,
}

/// Unconstrained text format. Always `text`.
///
/// Grammar format. Always `grammar`.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum FormatType {
    Grammar,
    Text,
}

/// Your chosen grammar.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct GrammarFormat {
    /// The grammar definition.
    pub definition: String,
    /// The syntax of the grammar definition. One of `lark` or `regex`.
    pub syntax: Syntax,
}

/// The syntax of the grammar definition. One of `lark` or `regex`.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum Syntax {
    Lark,
    Regex,
}

/// Usage statistics for the completion request.
///
/// An optional field that will only be present when you set
/// `stream_options: {"include_usage": true}` in your request. When present, it
/// contains a null value **except for the last chunk** which contains the
/// token usage statistics for the entire request.
///
/// **NOTE:** If the stream is interrupted or cancelled, you may not
/// receive the final usage chunk which contains the total token usage for
/// the request.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct CompletionUsage {
    /// Number of tokens in the generated completion.
    pub completion_tokens: i64,
    /// Breakdown of tokens used in a completion.
    pub completion_tokens_details: Option<CompletionTokensDetails>,
    /// Number of tokens in the prompt.
    pub prompt_tokens: i64,
    /// Breakdown of tokens used in the prompt.
    pub prompt_tokens_details: Option<PromptTokensDetails>,
    /// Total number of tokens used in the request (prompt + completion).
    pub total_tokens: i64,
}

/// Constrains the verbosity of the model's response. Lower values will result in
/// more concise responses, while higher values will result in more verbose responses.
/// Currently supported values are `low`, `medium`, and `high`.
///
///
/// High level guidance for the amount of context window space to use for the
/// search. One of `low`, `medium`, or `high`. `medium` is the default.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum Verbosity {
    High,
    Low,
    Medium,
}

/// This tool searches the web for relevant results to use in a response.
/// Learn more about the [web search
/// tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct WebSearch {
    pub search_context_size: Option<Verbosity>,
    /// Approximate location parameters for the search.
    pub user_location: Option<UserLocation>,
}

/// Approximate location parameters for the search.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct UserLocation {
    pub approximate: WebSearchLocation,
    /// The type of location approximation. Always `approximate`.
    #[serde(rename = "type")]
    pub user_location_type: UserLocationType,
}

/// Approximate location parameters for the search.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub struct WebSearchLocation {
    /// Free text input for the city of the user, e.g. `San Francisco`.
    pub city: Option<String>,
    /// The two-letter
    /// [ISO country code](https://en.wikipedia.org/wiki/ISO_3166-1) of the user,
    /// e.g. `US`.
    pub country: Option<String>,
    /// Free text input for the region of the user, e.g. `California`.
    pub region: Option<String>,
    /// The [IANA timezone](https://timeapi.io/documentation/iana-timezones)
    /// of the user, e.g. `America/Los_Angeles`.
    pub timezone: Option<String>,
}

/// The type of location approximation. Always `approximate`.
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum UserLocationType {
    Approximate,
}
